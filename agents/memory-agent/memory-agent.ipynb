{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7311d272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import getpass\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9645faf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google AI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ebaf72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(model = \"gemini-2.5-flash-preview-05-20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b10185e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "  messages: list[HumanMessage | AIMessage]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82f00ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_node(state:AgentState) -> AgentState:\n",
    "  response = llm.invoke(state['messages'])\n",
    "  state['messages'].append(AIMessage(content=response.content))\n",
    "  print(f\"AI: {response.content}\\n\")\n",
    "  return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2e988e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(state_schema=AgentState)\n",
    "graph.add_node(\"chatbot\",chat_node)\n",
    "graph.add_edge(START,\"chatbot\")\n",
    "graph.add_edge(\"chatbot\",END)\n",
    "\n",
    "agent = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65014bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x000001D7BDC72510>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "890d0ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e7769c",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_speaker = None\n",
    "current_content = []\n",
    "\n",
    "with open(\"conversation_history.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line.startswith(\"You: \"):\n",
    "            # Save the previous message before starting a new one\n",
    "            if current_speaker == \"You\" and current_content:\n",
    "                conversation_history.append(HumanMessage(content=\"\\n\".join(current_content)))\n",
    "            elif current_speaker == \"AI\" and current_content:\n",
    "                conversation_history.append(AIMessage(content=\"\\n\".join(current_content)))\n",
    "            \n",
    "            current_speaker = \"You\"\n",
    "            current_content = [line[len(\"You: \"):]]\n",
    "        \n",
    "        elif line.startswith(\"AI: \"):\n",
    "            if current_speaker == \"You\" and current_content:\n",
    "                conversation_history.append(HumanMessage(content=\"\\n\".join(current_content)))\n",
    "            elif current_speaker == \"AI\" and current_content:\n",
    "                conversation_history.append(AIMessage(content=\"\\n\".join(current_content)))\n",
    "            \n",
    "            current_speaker = \"AI\"\n",
    "            current_content = [line[len(\"AI: \"):]]\n",
    "        \n",
    "        else:\n",
    "            # Line is part of the current message\n",
    "            current_content.append(line)\n",
    "\n",
    "# Don't forget to append the last message\n",
    "if current_speaker == \"You\" and current_content:\n",
    "    conversation_history.append(HumanMessage(content=\"\\n\".join(current_content)))\n",
    "elif current_speaker == \"AI\" and current_content:\n",
    "    conversation_history.append(AIMessage(content=\"\\n\".join(current_content)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9587c2bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hi, I am Shorya', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Hello Shorya! Nice to meet you.\\n\\nI'm an AI assistant. How can I help you today?\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Hmm, I am learning langgraph, using gemini as an LLM client, how can i add an systemt prompt to it?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='That\\'s a great question, and it\\'s a common pattern when building agents with LangGraph!\\n\\nWhen using Gemini via LangChain (which LangGraph leverages), you add a system prompt by including a `SystemMessage` object at the beginning of the list of messages you pass to the LLM.\\n\\nHere\\'s how you can do it within a LangGraph node:\\n\\n### Core Concept: The `SystemMessage`\\n\\nLangChain\\'s message types (which Gemini understands) include:\\n*   `HumanMessage`: For user input.\\n*   `AIMessage`: For AI responses.\\n*   `SystemMessage`: For instructions, persona, or context that guides the AI\\'s behavior.\\n\\nYou pass a list of these messages to your LLM. The `SystemMessage` should typically be the first message in the list.\\n\\n### Example Implementation\\n\\nLet\\'s set up a simple LangGraph with a node that calls Gemini and adds a system prompt.\\n\\nFirst, make sure you have the necessary libraries installed:\\n```bash\\npip install -U langgraph langchain_google_genai langchain\\n```\\n\\nAnd set your Google API key:\\n```python\\nimport os\\nos.environ[\"GOOGLE_API_KEY\"] = \"YOUR_GEMINI_API_KEY\"\\n```\\n\\nNow, the LangGraph code:\\n\\n```python\\nfrom typing import TypedDict, List\\nfrom langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage\\nfrom langchain_google_genai import ChatGoogleGenerativeAI\\nfrom langgraph.graph import StateGraph, END\\n\\n# 1. Define the Graph State\\n# This defines what data will flow through your graph.\\n# We need a list of messages to maintain conversation history.\\nclass AgentState(TypedDict):\\nmessages: List[BaseMessage]\\n\\n# 2. Initialize your LLM\\nllm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\\n\\n# 3. Define the System Prompt\\n# This is the instruction you want to give to Gemini.\\nSYSTEM_PROMPT_TEXT = \"You are a helpful and very concise assistant. Always provide short answers, preferably one sentence.\"\\n\\n# 4. Define the LLM Node Function\\n# This function will be a node in your LangGraph.\\n# It takes the current state, adds the system prompt, calls the LLM, and returns the AI\\'s response.\\ndef call_llm(state: AgentState) -> dict:\\nprint(\"---CALLING LLM NODE---\")\\nmessages = state[\\'messages\\']\\n\\n# Prepend the SystemMessage to the list of messages\\n# This is the key step!\\nmessages_with_system_prompt = [SystemMessage(content=SYSTEM_PROMPT_TEXT)] + messages\\n\\n# Invoke the LLM with the combined messages\\nresponse = llm.invoke(messages_with_system_prompt)\\n\\n# Return the AI\\'s response to update the state\\nreturn {\"messages\": [response]}\\n\\n# 5. Build the LangGraph\\nworkflow = StateGraph(AgentState)\\n\\n# Add the LLM node\\nworkflow.add_node(\"llm_node\", call_llm)\\n\\n# Set the entry point of the graph\\nworkflow.set_entry_point(\"llm_node\")\\n\\n# Define the edge from the LLM node to the end of the graph\\nworkflow.add_edge(\"llm_node\", END)\\n\\n# Compile the graph\\napp = workflow.compile()\\n\\n# 6. Run the Graph\\nprint(\"\\\\n---RUN 1: Normal Question---\")\\ninputs1 = {\"messages\": [HumanMessage(content=\"What is the capital of France?\")]}\\nfor s in app.stream(inputs1):\\nprint(s)\\n# Expected: \"Paris.\" (or similar concise answer due to system prompt)\\n\\nprint(\"\\\\n---RUN 2: Question where conciseness matters---\")\\ninputs2 = {\"messages\": [HumanMessage(content=\"Tell me about the history of the internet.\")]}\\nfor s in app.stream(inputs2):\\nprint(s)\\n# Expected: A very short summary, not a long paragraph, due to the system prompt.\\n```\\n\\n### Explanation of the Key Part:\\n\\n```python\\nmessages = state[\\'messages\\']\\n# Prepend the SystemMessage to the list of messages\\nmessages_with_system_prompt = [SystemMessage(content=SYSTEM_PROMPT_TEXT)] + messages\\nresponse = llm.invoke(messages_with_system_prompt)\\n```\\n\\n1.  We retrieve the existing `messages` from the current `state`. These `messages` typically contain the `HumanMessage` from the user and any previous `AIMessage`s if it\\'s a multi-turn conversation.\\n2.  We create a `SystemMessage` instance using our defined `SYSTEM_PROMPT_TEXT`.\\n3.  We then *prepend* this `SystemMessage` to the `messages` list using `[SystemMessage(...)] + messages`. This ensures the system instruction is the very first thing Gemini \"sees\" when processing the prompt.\\n4.  Finally, `llm.invoke()` is called with this new list of messages, incorporating the system prompt.\\n\\n### Advanced Considerations:\\n\\n*   **Dynamic System Prompts:** If your system prompt needs to change based on the user\\'s intent or previous turns, you could store the system prompt text in your `AgentState` and modify it with a preceding node.\\n```python\\nclass AgentState(TypedDict):\\nmessages: List[BaseMessage]\\nsystem_prompt: str # Add this\\n\\n# In your initial state:\\ninitial_state = {\"messages\": [HumanMessage(content=\"What is the capital of France?\")], \"system_prompt\": \"You are a helpful assistant.\"}\\n\\n# In your call_llm node:\\ndef call_llm(state: AgentState) -> dict:\\nmessages = state[\\'messages\\']\\ncurrent_system_prompt = state.get(\\'system_prompt\\', \"You are a general AI assistant.\") # Fallback\\nmessages_with_system_prompt = [SystemMessage(content=current_system_prompt)] + messages\\n# ... rest of the code\\n```\\n*   **Multiple LLM Nodes:** If you have different LLM nodes for different tasks within your graph, each can have its own specific system prompt tailored to its function.\\n*   **LangChain Expression Language (LCEL):** For more complex prompt engineering (e.g., using `ChatPromptTemplate` with variables), you can compose your prompt chain using LCEL before passing it to the LLM. However, for just adding a static system prompt, the direct `SystemMessage` approach is often sufficient and clear.\\n\\nThis method gives you precise control over the initial instructions given to your Gemini model within your LangGraph application.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "142b594d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: Hi, I am Shorya\n",
      "AI: Hello Shorya! Nice to meet you.\n",
      "\n",
      "I'm an AI assistant. How can I help you today?\n",
      "\n",
      "You: Hmm, I am learning langgraph, using gemini as an LLM client, how can i add an systemt prompt to it?\n",
      "AI: That's a great question, and it's a common pattern when building agents with LangGraph!\n",
      "\n",
      "When using Gemini via LangChain (which LangGraph leverages), you add a system prompt by including a `SystemMessage` object at the beginning of the list of messages you pass to the LLM.\n",
      "\n",
      "Here's how you can do it within a LangGraph node:\n",
      "\n",
      "### Core Concept: The `SystemMessage`\n",
      "\n",
      "LangChain's message types (which Gemini understands) include:\n",
      "*   `HumanMessage`: For user input.\n",
      "*   `AIMessage`: For AI responses.\n",
      "*   `SystemMessage`: For instructions, persona, or context that guides the AI's behavior.\n",
      "\n",
      "You pass a list of these messages to your LLM. The `SystemMessage` should typically be the first message in the list.\n",
      "\n",
      "### Example Implementation\n",
      "\n",
      "Let's set up a simple LangGraph with a node that calls Gemini and adds a system prompt.\n",
      "\n",
      "First, make sure you have the necessary libraries installed:\n",
      "```bash\n",
      "pip install -U langgraph langchain_google_genai langchain\n",
      "```\n",
      "\n",
      "And set your Google API key:\n",
      "```python\n",
      "import os\n",
      "os.environ[\"GOOGLE_API_KEY\"] = \"YOUR_GEMINI_API_KEY\"\n",
      "```\n",
      "\n",
      "Now, the LangGraph code:\n",
      "\n",
      "```python\n",
      "from typing import TypedDict, List\n",
      "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage\n",
      "from langchain_google_genai import ChatGoogleGenerativeAI\n",
      "from langgraph.graph import StateGraph, END\n",
      "\n",
      "# 1. Define the Graph State\n",
      "# This defines what data will flow through your graph.\n",
      "# We need a list of messages to maintain conversation history.\n",
      "class AgentState(TypedDict):\n",
      "    messages: List[BaseMessage]\n",
      "\n",
      "# 2. Initialize your LLM\n",
      "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
      "\n",
      "# 3. Define the System Prompt\n",
      "# This is the instruction you want to give to Gemini.\n",
      "SYSTEM_PROMPT_TEXT = \"You are a helpful and very concise assistant. Always provide short answers, preferably one sentence.\"\n",
      "\n",
      "# 4. Define the LLM Node Function\n",
      "# This function will be a node in your LangGraph.\n",
      "# It takes the current state, adds the system prompt, calls the LLM, and returns the AI's response.\n",
      "def call_llm(state: AgentState) -> dict:\n",
      "    print(\"---CALLING LLM NODE---\")\n",
      "    messages = state['messages']\n",
      "\n",
      "    # Prepend the SystemMessage to the list of messages\n",
      "    # This is the key step!\n",
      "    messages_with_system_prompt = [SystemMessage(content=SYSTEM_PROMPT_TEXT)] + messages\n",
      "\n",
      "    # Invoke the LLM with the combined messages\n",
      "    response = llm.invoke(messages_with_system_prompt)\n",
      "\n",
      "    # Return the AI's response to update the state\n",
      "    return {\"messages\": [response]}\n",
      "\n",
      "# 5. Build the LangGraph\n",
      "workflow = StateGraph(AgentState)\n",
      "\n",
      "# Add the LLM node\n",
      "workflow.add_node(\"llm_node\", call_llm)\n",
      "\n",
      "# Set the entry point of the graph\n",
      "workflow.set_entry_point(\"llm_node\")\n",
      "\n",
      "# Define the edge from the LLM node to the end of the graph\n",
      "workflow.add_edge(\"llm_node\", END)\n",
      "\n",
      "# Compile the graph\n",
      "app = workflow.compile()\n",
      "\n",
      "# 6. Run the Graph\n",
      "print(\"\\n---RUN 1: Normal Question---\")\n",
      "inputs1 = {\"messages\": [HumanMessage(content=\"What is the capital of France?\")]}\n",
      "for s in app.stream(inputs1):\n",
      "    print(s)\n",
      "# Expected: \"Paris.\" (or similar concise answer due to system prompt)\n",
      "\n",
      "print(\"\\n---RUN 2: Question where conciseness matters---\")\n",
      "inputs2 = {\"messages\": [HumanMessage(content=\"Tell me about the history of the internet.\")]}\n",
      "for s in app.stream(inputs2):\n",
      "    print(s)\n",
      "# Expected: A very short summary, not a long paragraph, due to the system prompt.\n",
      "```\n",
      "\n",
      "### Explanation of the Key Part:\n",
      "\n",
      "```python\n",
      "    messages = state['messages']\n",
      "    # Prepend the SystemMessage to the list of messages\n",
      "    messages_with_system_prompt = [SystemMessage(content=SYSTEM_PROMPT_TEXT)] + messages\n",
      "    response = llm.invoke(messages_with_system_prompt)\n",
      "```\n",
      "\n",
      "1.  We retrieve the existing `messages` from the current `state`. These `messages` typically contain the `HumanMessage` from the user and any previous `AIMessage`s if it's a multi-turn conversation.\n",
      "2.  We create a `SystemMessage` instance using our defined `SYSTEM_PROMPT_TEXT`.\n",
      "3.  We then *prepend* this `SystemMessage` to the `messages` list using `[SystemMessage(...)] + messages`. This ensures the system instruction is the very first thing Gemini \"sees\" when processing the prompt.\n",
      "4.  Finally, `llm.invoke()` is called with this new list of messages, incorporating the system prompt.\n",
      "\n",
      "### Advanced Considerations:\n",
      "\n",
      "*   **Dynamic System Prompts:** If your system prompt needs to change based on the user's intent or previous turns, you could store the system prompt text in your `AgentState` and modify it with a preceding node.\n",
      "    ```python\n",
      "    class AgentState(TypedDict):\n",
      "        messages: List[BaseMessage]\n",
      "        system_prompt: str # Add this\n",
      "\n",
      "    # In your initial state:\n",
      "    initial_state = {\"messages\": [HumanMessage(content=\"What is the capital of France?\")], \"system_prompt\": \"You are a helpful assistant.\"}\n",
      "\n",
      "    # In your call_llm node:\n",
      "    def call_llm(state: AgentState) -> dict:\n",
      "        messages = state['messages']\n",
      "        current_system_prompt = state.get('system_prompt', \"You are a general AI assistant.\") # Fallback\n",
      "        messages_with_system_prompt = [SystemMessage(content=current_system_prompt)] + messages\n",
      "        # ... rest of the code\n",
      "    ```\n",
      "*   **Multiple LLM Nodes:** If you have different LLM nodes for different tasks within your graph, each can have its own specific system prompt tailored to its function.\n",
      "*   **LangChain Expression Language (LCEL):** For more complex prompt engineering (e.g., using `ChatPromptTemplate` with variables), you can compose your prompt chain using LCEL before passing it to the LLM. However, for just adding a static system prompt, the direct `SystemMessage` approach is often sufficient and clear.\n",
      "\n",
      "This method gives you precise control over the initial instructions given to your Gemini model within your LangGraph application.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_input = input(\"Enter your message: \")\n",
    "\n",
    "while user_input.lower() != \"exit\":\n",
    "    conversation_history.append(HumanMessage(content=user_input))\n",
    "    \n",
    "    print(f\"You: {user_input}\")\n",
    "    response = agent.invoke({\"messages\":conversation_history})\n",
    "    \n",
    "    conversation_history = response['messages']\n",
    "    \n",
    "    user_input = input(\"Enter your message: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7d15663",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"conversation_history.txt\", \"w\") as f:\n",
    "    for message in conversation_history:\n",
    "        if isinstance(message, HumanMessage):\n",
    "            f.write(f\"You: {message.content}\\n\")\n",
    "        elif isinstance(message, AIMessage):\n",
    "            f.write(f\"AI: {message.content}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
